<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="🦎 Yo'Chameleon: Personalized Vision and Language Generation">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="./static/images/yochameleon-without-bg.png" />
  <meta property="og:title" content="🦎 Yo'Chameleon: Personalized Vision and Language Generation" />
  <meta property="og:description" content="Yo'Chameleon: Personalized Vision and Language Generation!" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title>Yo'Chameleon</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/yochameleon-without-bg.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }
  .center {
    margin-left: auto;
    margin-right: auto;
  }
  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="color:#06402b;">🦎 Yo'Chameleon 🦎</h1>
            <h3 class="title is-2 publication-title"><text style="color:#06402b;">Personalized</text> <text style="color:#cd1c18;">Vision</text> <text style="color:#06402b;">and <text><text style="color:#000080;">Language</text> <text style="color:#06402b;">Generation<text></h3>
            <h5 class="subtitle is-4 publication-awards">——— CVPR 2025 ———</h5>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <!-- <br> -->
                <a href="https://thaoshibe.github.io/" style="color:#f68946;font-weight:normal;">Thao Nguyen<sup>1✨</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://krsingh.cs.ucdavis.edu/" style="color:#f68946;font-weight:normal;">Krishna Kumar Singh<sup>2</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://jshi31.github.io/jingshi/" style="color:#008AD7;font-weight:normal;">Jing Shi<sup>2</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/trungbuistanford/" style="color:#F2A900;font-weight:normal;">Trung Bui<sup>2</sup></a>
                &nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee<sup>1, 🚩</sup></a>
                  &nbsp;&nbsp;&nbsp;
              </span>
              <span class="author-block">
                <a href="https://yuheng-li.github.io/" style="color:#F2A900;font-weight:normal;">Yuheng Li<sup>2, 🚩</sup></a>
                <!-- &nbsp;&nbsp;&nbsp; -->
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <div>
                <a href="https://www.cs.wisc.edu/">
                  <img src="./static/images/uwmadison-logo.png" style="height: 60px;">
                </a>
                &nbsp;&nbsp;&nbsp;
                <a href="https://research.adobe.com/">
                  <img src="./static/images/adobe-logo.png" style="height: 40px;">
                </a>
<!--                 &nbsp;&nbsp;&nbsp;
                <a href="https://www.cs.wisc.edu/">
                  <img src="./static/images/uwmadison-logo.png" style="height: 60px;">
                </a> -->
              </div>
              <span class="author-block">1. University of Wisconsin-Madison</span>
              &nbsp;&nbsp;&nbsp;
              <span class='author-block'>2. Adobe Research</span>
              <br>
              <small><i>🚩: equal advising</i></small>

            </div>
            <div><a href="./static/images/yochameleon-without-bg.png"><img src='./static/images/yochameleon-without-bg.png' width="200"></a></div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="ARXIV LINK" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/thaoshibe/YoChameleon" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/thaoshibe/YoChameleon" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
<!--                 <span class="link-block">
                  <a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="POSTER" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>Poster (Updating)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <h4 class="subtitle has-text-centered"> -->
          <center><img src='./static/images/yochameleon-bo.png' width="2000"></center>
          <br>
          <!-- <text style="font-size:110%">Given just a few images of a novel subject (e.g., a dog named <text style="font-family: Courier;color:#ff8c00;">&lt;bo&gt;</text>), Yo’LLaVA learns to facilitate textual/visual conversations centered around that subject.</text> -->
          <text style="font-size:110%">Using only 3-5 images of a novel concept/subject, we personalize Large Multimodal Models (e.g., Chameleon) so that they retain their original capabilities while enabling tailored language and vision generation for the novel concept.
          </text>
        <!-- </h4> -->
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#f3f3f3">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">📜 Abstract</h2>
          <div class="content has-text-justified">
            <p>
                  <i>[Motivation]</i><br>
                  Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation.
                  <br>
                  In this paper, <b>we introduce Yo'Chameleon, the first attempt to study personalization for Large Multimodal Models</b>.
                  <br>
                  <br>
                  <i>[Problem Statement]</i><br>
                  Given 3-5 images of a particular concept, Yo'Chameleon leverages <b><text style="color:green;'">soft-prompt tuning</text></b> to embed subject-specific information to:
                  <br>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (i) answer questions about the subject (<i>personalized language generation</i>)
                  <br>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and (ii) recreate pixel-level details to produce images of the subject in new contexts (<i>personalized vision generation</i>)
                  <br><br>
                  <i>[Approach/ Results]</i><br>
                  Yo'Chameleon is trained with:
                  <br>(i) <b><text style="color:green">a self-prompting optimization mechanism</text></b> to balance performance across multiple modalities,
                  <br>and (ii) <b><text style="color:green;'">a ``soft-positive" image generation approach</text></b> to enhance image quality in a few-shot setting.
                  <br>
                  Our qualitative and quantitative analyses reveal that Yo’Chameleon can learn concepts more efficiently using fewer tokens and effectively encode visual attributes, outperforming prompting baselines.
           </p>
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">🦎 Yo'Chameleon (3S): Soft-prompt, Soft-positive & Self-prompting</h2>
    </div>
  </div>
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
    <table class="center" style="max-width:1100px;">
        <tr>
            <td width="35%">
                <img id="teaser"src="./static/images/framework.png" alt="Training Pipeline">
            </td>
            <td>
                <b style="font-size:120%;background-color: lightyellow;">⚙️ Soft-prompt:</b><br>
                <p>We define a personalized soft-prompt for the subject as:</p>
                <center>
                <div class="quote">
                    "<b><text style="font-family: Courier; color:blue">&lt;sks&gt;</text></b> is <b><text style="font-family: Courier; color:red">&lt;g-tokens&gt</text></text><text style="font-family: Courier;color:green">&lt;u-tokens&gt</text></b>"
                </div>
                </center>
                <br>
                <p>Where:
                  <br>
                  • <b><text style="font-family: Courier; color:blue">&lt;sks&gt;</text></b>: an identifier for the subject
                  <br>
                  • <b><text style="font-family: Courier; color:red">&lt;g-tokens&gt</text></b> denotes <i>k</i> latent tokens used for personalized image generation task (<text style="font-family: Courier; color:red">&lt;token<sub>1</sub>&gt...</text><text style="font-family: Courier;color:red">&lt;token<sub>k</sub>&gt</text></b>)
                  <br>
                  • <b><text style="font-family: Courier; color:green">&lt;u-tokens&gt</text></b> denotes <i>h</i> latent tokens used for personalized image generation task (<text style="font-family: Courier; color:green">&lt;token<sub>1</sub>&gt...</text><text style="font-family: Courier;color:green">&lt;token<sub>h</sub>&gt</text></b>)
                  <br>
            </td>
        </tr>
        <tr>
          <br>
          <td>
            <b style="font-size:120%;background-color: lightyellow;">🌱 "Soft positive" images</b>:
            <br><br>
            To overcome the limited number of training images (3-5 images), we propose to use “soft-positive” images with dynamic prompt length to enhance the image generation quality.

            <!-- The key insight is that hard negative images can share varying degrees of similar characteristics with the positive samples, and thus should contribute differently to the learning process. -->
            <br><br>
            <center><img src="./static/images/soft-positive.png" alt="soft-positive-images" style="width: 450px; height: auto;"></center>
            <i>“Soft positive” images.
            <!-- Retrieved images are ranked according to their similarity to positive images using CLIP image similarity scores. -->
            Images that are more similar to the actual positive images are described with more latent tokens (i.e., more details).</i>

          </td>
          <td width="30%">
            <b style="font-size:120%;background-color: lightyellow;">🌱 Self-Prompting</b>:
            <br><br>
            To balance the performance across the modality (language generation and image generation), we propose: (i) use two set of soft prompts and (ii) self-prompting optimization techniques. 

            <!-- The key insight is that hard negative images can share varying degrees of similar characteristics with the positive samples, and thus should contribute differently to the learning process. -->
            <br><br>
            <center><img src="./static/images/selfprompting.png" alt="selfprompting" style="width: 500px; height: auto;"></center>
            <i>Self-prompting mechanism. When multiple tasks are presented, the model first predicts which information (latent tokens) should be used for this task first, and then performs the task.</i>
          </td>
        </tr>
    </table>

      <centering>
      </centering>           
    </div>
  </div>
</section>

<hr>
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">🖼️ Qualitative Result</h2>
          <center><img src="./static/images/qual1.png"></center>
          <br><br>
          <center><img src="./static/images/qual2.png"></center>
        </div>
      </div>
    </div>
  </section>
<hr>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">📈 Additional "Qualitative" Experiements</h2>
    </div>
  </div>
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
    <table class="center" style="max-width:1100px;">
        <tr>
            <td width="30%">
                <b style="font-size:120%;">📊 Full-model Finetuning vs. Soft Prompt</b>
                <br><br>
                In this experiment, our goal is to verify whether soft prompt tuning could achieve performance comparable to full- model fine-tuning, which is commonly used in personalized image generation.
                <br>
                <br>
                We collected photos for three concepts: one person (300 images), one dog (500 images), and one cat (500 images).
                <br><br>
                Qualitative result (right) effectively demonstrates the advantages of soft prompt tuning over full-model fine-tuning: (1) it matches the performance of full-model fine-tuning in personalized tasks and (2) mitigates catastrophic forgetting.
                <br>
                (Quantitative table can be found in Supplementary)
            </td>
            <td>
                <center><img id="teaser"src="./static/images/softprompt-vs-fullmodel.png" alt="soft prompt vs fullmodel" style="width: 450px; height: auto;"></center>
            </td>
        </tr>
        <tr>
          <br>
          <td>
            <b style="font-size:120%;">📊 Unbalanced performance across modalities</b>:
            <br><br>
            Optimized tokens for one task cannot effectively perform another, and simply training on a mixture of data yields suboptimal performance across tasks.
            <!-- We propose a self-prompting approach, where the model predicts which task to perform first, achieving the best of both worlds. -->
            <br><br>
            <center><img src="./static/images/modalities.png" alt="modalities" style="width: 500px; height: auto;"></center>
            <!-- <i>“Soft positive” images. -->
            <!-- Retrieved images are ranked according to their similarity to positive images using CLIP image similarity scores. -->
            <!-- Images that are more similar to the actual positive images are described with more latent tokens (i.e., more details).</i> -->

          </td>
          <td width="30%">
            <b style="font-size:120%;">📊 Limitations </b>:
            <br><br>
            Our method is not without limitations.<br>
            - 1st: when dealing with objects that have intricate details (e.g., text on a cup or characters on a keyboard).<br>
            - 2nd: our method’s performance is constrained by the capabilities of the base model (i.e., multiple personalized concepts generation).<br>
            - Lastly, there remains a significant gap for personalizing human faces.
            <br><br>
            <center><img src="./static/images/limitation.png" alt="limitation" style="width: 500px; height: auto;"></center>
          </td>
        </tr>
    </table>

      <centering>
      </centering>           
    </div>
  </div>
</section>

<hr>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">📥 BibTeX</h2>
      <pre><code>
@article{yochameleon,
  title={Yo'Chameleon: Personalized Vision and Language Generation},
  author={Thao Nguyen and Krishna Kumar Singh and Jing Shi and Trung Bui and Yong Jae Lee and Yuheng Li},
  journal={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025},
}
  </code></pre>
    </div>
<center>📣 Related Works on Personalization for LMMs can be found via this <b><a href="https://github.com/thaoshibe/awesome-personalized-lmms?tab=readme-ov-file#papers" target="_blank">[📣 Awesome Personalized LMMs -- GitHub]</a></b></center>

  </section>
  
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">💌 Acknowledgement</h2>
  <p>
I would like to express my gratitude to my Adobe Research's mentors: Dr. Krishna, Dr. Jing Shi, and Dr. Trung Bui for their discussions. Special thanks to my advisor, Prof. Yong Jae Lee, who provided endless insights and guidance for this project (as always).

A big shout-out to my primary fellow mentee Sicheng Mo—he taught me so much about coding. Without him, I’d still be using TensorBoard instead of WanDB! (Also, he has wonderful taste in food and restaurants.)
Additionally, thanks to (technically-not) mentor Fangzhou Mu for hosting many Friday dinners and board game nights during the summer 🥓🍣🍱 (though, he’s not a fan of Thai foods —meh~).

And finally, saving the best for last: I couldn’t have completed this project without the unwavering support (and pushes) of my main Adobe juan mentor, Dr. Yuheng Li :xixi:. Thank you so much!
  </p>

  <p>
  <!-- <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. -->
  </p>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    <center>
      <i>Thank you (.❛ ᴗ ❛.).</i>
      
    <center>
    <a href="javascript:toggleblock('notice')" style="text-align:center;font-size:70%;color:#808080">▶ thaoshibe.github.io's clustrmaps 🌎</a>.
    <div id="notice" style="display:none; color:#dedede; font-size:.5em;">
      <p>
        <a href="https://clustrmaps.com/site/1c3j9"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=M0hZnymEZ1MpTZi6F8Wfm3ZJX6qPdo3fHOaZ2wQAZbE&cl=ffffff" /></a>
      </p>
    </div>
  </center>
    </center>
      <p>
      </p>    
    </div>
  </section>
</body>

</html>
